{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb255563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "\n",
    "from scipy import stats\n",
    "import scipy.linalg as linalg\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89ccdb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denom bias: 0.021506523529193394\n",
      "num bias: 0.007307873257857934\n",
      "llr: 0.38733691667435627 omega0: 0.02701296357125336 fixed 0.14911815359001485 test_stat: 0.08214069508434663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OLS_loglike(GenericLikelihoodModel):\n",
    "    \n",
    "    def __init__(self, *args,ols=False, **kwargs):\n",
    "        super(OLS_loglike,self).__init__(*args,**kwargs)\n",
    "        self.ols = ols\n",
    "\n",
    "    def loglikeobs(self, params):\n",
    "        y = self.endog\n",
    "        x = self.exog\n",
    "        mu_y = np.matmul(x,params)  \n",
    "        resid = y - mu_y\n",
    "        sigma = np.sqrt(np.sum(resid**2)/resid.shape[0])\n",
    "        pr_y = stats.norm.logpdf( resid, loc=0,scale=sigma )\n",
    "        return pr_y\n",
    "\n",
    "def gen_data(nobs, num_cov, a):\n",
    "    xn = np.random.normal(scale=1., size=(nobs, num_cov))\n",
    "    e = np.random.normal(loc=0.0, scale=1.0, size=(nobs))\n",
    "    yn = (xn.sum(axis=1) * a/num_cov) + e\n",
    "    return yn, xn\n",
    "\n",
    "\n",
    "def setup_model(model1_fit,yn,xn):\n",
    "    \"\"\"setup models for ease\"\"\"\n",
    "    param1 = (model1_fit.params)\n",
    "    model1_deriv = OLS_loglike(yn,sm.add_constant(xn))\n",
    "    ll1 = model1_deriv.loglikeobs(model1_fit.params)\n",
    "    grad1 =  model1_deriv.score_obs(model1_fit.params)    \n",
    "    hess1 = model1_deriv.hessian(model1_fit.params)\n",
    "    return ll1,grad1,hess1,param1\n",
    "\n",
    "\n",
    "def setup_test(yn,xn,a=.1):\n",
    "    lls = []\n",
    "    grads = []\n",
    "    hesss = []\n",
    "    params = []\n",
    "    ols = sm.OLS(yn, sm.add_constant(xn)).fit()\n",
    "    ridge = sm.OLS(yn, sm.add_constant(xn)).fit_regularized(method='elastic_net', alpha=a, L1_wt=0.0)\n",
    "    for model in [ols,ridge]:\n",
    "        ll,grad,hess,param = setup_model(model,yn,xn)\n",
    "        lls.append(ll)\n",
    "        params.append(param)\n",
    "        grads.append(grad)\n",
    "        hesss.append(hess)   \n",
    "\n",
    "    return lls[0],grads[0],hesss[0],params[0],lls[1],grads[1],hesss[1],params[1]\n",
    "\n",
    "\n",
    "def compute_eigen2(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2):\n",
    "    \n",
    "    n = ll1.shape[0]\n",
    "    hess1 = hess1/n\n",
    "    hess2 = hess2/n\n",
    "\n",
    "    k1 = params1.shape[0]\n",
    "    k2 = params2.shape[0]\n",
    "    k = k1 + k2\n",
    "    \n",
    "    #A_hat:\n",
    "    A_hat1 = np.concatenate([hess1,np.zeros((k2,k1))])\n",
    "    A_hat2 = np.concatenate([np.zeros((k1,k2)),-1*hess2])\n",
    "    A_hat = np.concatenate([A_hat1,A_hat2],axis=1)\n",
    "\n",
    "    #B_hat, covariance of the score...\n",
    "    B_hat =  np.concatenate([grad1,-grad2],axis=1) #might be a mistake here..\n",
    "    B_hat = np.cov(B_hat.transpose())\n",
    "\n",
    "    #compute eigenvalues for weighted chisq\n",
    "    sqrt_B_hat= linalg.sqrtm(B_hat)\n",
    "    W_hat = np.matmul(sqrt_B_hat,linalg.inv(A_hat))\n",
    "    W_hat = np.matmul(W_hat,sqrt_B_hat)\n",
    "    V,W = np.linalg.eig(W_hat)\n",
    "\n",
    "    return V\n",
    "\n",
    "def compute_stage1(ll1,grad1,hess1,params1,ll2, grad2,hess2,params2):\n",
    "    nsims = 5000\n",
    "    \n",
    "    k1 = params1.shape[0]\n",
    "    k2 = params2.shape[0]\n",
    "    k = k1 + k2\n",
    "    \n",
    "    V = compute_eigen2(ll1,grad1,hess1,params1,ll2, grad2,hess2,params2)\n",
    "    np.random.seed()\n",
    "    Z0 = np.random.normal( size=(nsims,k) )**2\n",
    "    \n",
    "    return np.matmul(Z0,V*V)\n",
    "\n",
    "def two_step_test(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2,biascorrect=True):\n",
    "    stage1_distr = compute_stage1(ll1,grad1,hess1,params1,ll2, grad2,hess2,params2)\n",
    "    print('denom bias:', stage1_distr.mean())\n",
    "    nobs = ll1.shape[0]\n",
    "    omega0 = np.sqrt( (ll1 -ll2).var())\n",
    "    omega = np.sqrt( (ll1 -ll2).var() + stage1_distr.mean())#set up stuff\n",
    "    V =  compute_eigen2(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2)\n",
    "    llr = (ll1 - ll2).sum()\n",
    "    if biascorrect:\n",
    "        print('num bias:',V.sum()/2)\n",
    "        llr = llr + V.sum()/(2) #fix the test...\n",
    "    test_stat = llr/(omega*np.sqrt(nobs))\n",
    "    print('llr:',llr.sum(),'omega0:',omega0, 'fixed', omega, 'test_stat:',test_stat)\n",
    "    \n",
    "    stage1_res = ( nobs*omega**2 >= np.percentile(stage1_distr, 95, axis=0) )\n",
    "    return (1*(test_stat >= 1.96) + 2*( test_stat <= -1.96))*stage1_res\n",
    "\n",
    "y,X = gen_data(1000, 10, 1)\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_test(y,X)\n",
    "two_step_test(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X = gen_data(1000, 10, 5)\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_test(y,X)\n",
    "two_step_test(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2)\n",
    "y,X = gen_data(1000, 5, 1)\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_test(y,X)\n",
    "two_step_test(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2)\n",
    "y,X = gen_data(1000, 5, 5)\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_test(y,X)\n",
    "two_step_test(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf649227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denom bias: 0.14775883006698068\n",
      "num bias: 0.010855989484741615\n",
      "llr: 1.2616432089246492 omega0: 0.04801906006783997 fixed 0.3873818015818238 test_stat: 0.1029905410732842\n",
      "denom bias: 0.10458536820969887\n",
      "num bias: 0.012739838250595343\n",
      "llr: 0.8821830669704532 omega0: 0.040167906068793516 fixed 0.32588161790387976 test_stat: 0.08560494521916624\n",
      "denom bias: 1.5582900530930717\n",
      "num bias: -0.05965842081608089\n",
      "llr: 28.081708547971505 omega0: 0.23912839070104155 fixed 1.2710123682845662 test_stat: 0.6986726629612768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,X = gen_data(1000, 25, 1)\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_test(y,X,a=.2)\n",
    "two_step_test(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2)\n",
    "y,X = gen_data(1000, 25, 1)\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_test(y,X,a=.2)\n",
    "two_step_test(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2)\n",
    "y,X = gen_data(1000, 10, 5)\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_test(y,X,a=.2)\n",
    "two_step_test(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75e63013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denom bias: 21.686856067920786\n",
      "num bias: -0.10751639659489631\n",
      "llr: 41.946764129479156 omega0: 1.0247192437713488 fixed 4.768323143042648 test_stat: 0.27818440811115525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try it with seperate x?\n",
    "y,X = gen_data(1000, 10, 5)\n",
    "ll1,grad1,hess1,params1,ll2,grad2,hess2,params2 = setup_test(y,X,a=.2)\n",
    "y,X = gen_data(1000, 10, 5)\n",
    "_,_,_,_,ll2,grad2,hess2,params2 = setup_test(y,X,a=.2)\n",
    "two_step_test(ll1,grad1,hess1,params1,ll2,grad2,hess2,params2)\n",
    "#still a numerator bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d82b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
